{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Khiops Python samples_sklearn\n",
    "This is a notebook containing the code in the `samples_sklearn.py` script\nof the Khiops Python library.\n\nMake sure you have already installed the latest version of ",
    "[Khiops](http://www.khiops.org) before using this this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from os import path\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from khiops import core as kh\n",
    "from khiops.sklearn import (\n",
    "    KhiopsClassifier,\n",
    "    KhiopsCoclustering,\n",
    "    KhiopsEncoder,\n",
    "    KhiopsRegressor,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def khiops_classifier():\n",
    "    \"\"\"Trains a `.KhiopsClassifier` on a monotable dataframe\"\"\"\n",
    "    # Load the dataset into a pandas dataframe\n",
    "    adult_path = path.join(kh.get_samples_dir(), \"Adult\", \"Adult.txt\")\n",
    "    adult_df = pd.read_csv(adult_path, sep=\"\\t\")\n",
    "\n",
    "    # Split the whole dataframe into train and test (70%-30%)\n",
    "    adult_train_df, adult_test_df = train_test_split(\n",
    "        adult_df, test_size=0.3, random_state=1\n",
    "    )\n",
    "\n",
    "    # Split the dataset into:\n",
    "    # - the X feature table\n",
    "    # - the y target vector (\"class\" column)\n",
    "    X_train = adult_train_df.drop(\"class\", axis=1)\n",
    "    X_test = adult_test_df.drop(\"class\", axis=1)\n",
    "    y_train = adult_train_df[\"class\"]\n",
    "    y_test = adult_test_df[\"class\"]\n",
    "\n",
    "    # Create the classifier object\n",
    "    khc = KhiopsClassifier()\n",
    "\n",
    "    # Train the classifier\n",
    "    khc.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the classes on the test dataset\n",
    "    y_test_pred = khc.predict(X_test)\n",
    "    print(\"Predicted classes (first 10):\")\n",
    "    print(y_test_pred[0:10])\n",
    "    print(\"---\")\n",
    "\n",
    "    # Predict the class probabilities on the test dataset\n",
    "    y_test_probas = khc.predict_proba(X_test)\n",
    "    print(f\"Class order: {khc.classes_}\")\n",
    "    print(\"Predicted class probabilities (first 10):\")\n",
    "    print(y_test_probas[0:10])\n",
    "    print(\"---\")\n",
    "\n",
    "    # Evaluate accuracy and auc metrics on the test dataset\n",
    "    test_accuracy = metrics.accuracy_score(y_test, y_test_pred)\n",
    "    test_auc = metrics.roc_auc_score(y_test, y_test_probas[:, 1])\n",
    "    print(f\"Test accuracy = {test_accuracy}\")\n",
    "    print(f\"Test auc      = {test_auc}\")\n",
    "\n",
    "#Run sample\n",
    "khiops_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def khiops_classifier_multiclass():\n",
    "    \"\"\"Trains a multiclass `.KhiopsClassifier` on a monotable dataframe\"\"\"\n",
    "    # Load the dataset into a pandas dataframe\n",
    "    iris_path = path.join(kh.get_samples_dir(), \"Iris\", \"Iris.txt\")\n",
    "    iris_df = pd.read_csv(iris_path, sep=\"\\t\")\n",
    "\n",
    "    # Split the whole dataframe into train and test (70%-30%)\n",
    "    iris_train_df, iris_test_df = train_test_split(\n",
    "        iris_df, test_size=0.3, random_state=1\n",
    "    )\n",
    "\n",
    "    # Split the dataset into:\n",
    "    # - the X feature table\n",
    "    # - the y target vector (\"Class\" column)\n",
    "    X_train = iris_train_df.drop(\"Class\", axis=1)\n",
    "    X_test = iris_test_df.drop(\"Class\", axis=1)\n",
    "    y_train = iris_train_df[\"Class\"]\n",
    "    y_test = iris_test_df[\"Class\"]\n",
    "\n",
    "    # Create the classifier object\n",
    "    khc = KhiopsClassifier()\n",
    "\n",
    "    # Train the classifier\n",
    "    khc.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the classes on the test dataset\n",
    "    y_test_pred = khc.predict(X_test)\n",
    "    print(\"Predicted classes (first 10):\")\n",
    "    print(y_test_pred[:10])\n",
    "    print(\"---\")\n",
    "\n",
    "    # Predict the class probabilities on the test datasets\n",
    "    y_test_probas = khc.predict_proba(X_test)\n",
    "    print(f\"Class order: {khc.classes_}\")\n",
    "    print(\"Predicted class probabilities (first 10):\")\n",
    "    print(y_test_probas[:10])\n",
    "    print(\"---\")\n",
    "\n",
    "    # Evaluate accuracy and auc metrics on the test dataset\n",
    "    test_accuracy = metrics.accuracy_score(y_test, y_test_pred)\n",
    "    test_auc = metrics.roc_auc_score(y_test, y_test_probas, multi_class=\"ovr\")\n",
    "    print(f\"Test accuracy = {test_accuracy}\")\n",
    "    print(f\"Test auc      = {test_auc}\")\n",
    "\n",
    "#Run sample\n",
    "khiops_classifier_multiclass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def khiops_classifier_multitable_star():\n",
    "    \"\"\"Trains a `.KhiopsClassifier` on a star multi-table dataset\"\"\"\n",
    "    # Load the root table of the dataset into a pandas dataframe\n",
    "    accidents_dataset_path = path.join(kh.get_samples_dir(), \"AccidentsSummary\")\n",
    "    accidents_df = pd.read_csv(\n",
    "        path.join(accidents_dataset_path, \"Accidents.txt\"),\n",
    "        sep=\"\\t\",\n",
    "        encoding=\"latin1\",\n",
    "    )\n",
    "\n",
    "    # Split the root dataframe into train and test\n",
    "    accidents_train_df, accidents_test_df = train_test_split(\n",
    "        accidents_df, test_size=0.3, random_state=1\n",
    "    )\n",
    "\n",
    "    # Obtain the main X feature table and the y target vector (\"Class\" column)\n",
    "    y_train = accidents_train_df[\"Gravity\"]\n",
    "    y_test = accidents_test_df[\"Gravity\"]\n",
    "    X_train_main = accidents_train_df.drop(\"Gravity\", axis=1)\n",
    "    X_test_main = accidents_test_df.drop(\"Gravity\", axis=1)\n",
    "\n",
    "    # Load the secondary table of the dataset into a pandas dataframe\n",
    "    vehicles_df = pd.read_csv(\n",
    "        path.join(accidents_dataset_path, \"Vehicles.txt\"), sep=\"\\t\"\n",
    "    )\n",
    "\n",
    "    # Split the secondary dataframe with the keys of the splitted root dataframe\n",
    "    X_train_ids = X_train_main[\"AccidentId\"].to_frame()\n",
    "    X_test_ids = X_test_main[\"AccidentId\"].to_frame()\n",
    "    X_train_secondary = X_train_ids.merge(vehicles_df, on=\"AccidentId\")\n",
    "    X_test_secondary = X_test_ids.merge(vehicles_df, on=\"AccidentId\")\n",
    "\n",
    "    # Create the dataset multitable specification for the train/test split\n",
    "    # We specify each table with a name and a tuple (dataframe, key_columns)\n",
    "    X_train = {\n",
    "        \"main_table\": \"Accidents\",\n",
    "        \"tables\": {\n",
    "            \"Accidents\": (X_train_main, \"AccidentId\"),\n",
    "            \"Vehicles\": (X_train_secondary, [\"AccidentId\", \"VehicleId\"]),\n",
    "        },\n",
    "    }\n",
    "    X_test = {\n",
    "        \"main_table\": \"Accidents\",\n",
    "        \"tables\": {\n",
    "            \"Accidents\": (X_test_main, \"AccidentId\"),\n",
    "            \"Vehicles\": (X_test_secondary, [\"AccidentId\", \"VehicleId\"]),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Train the classifier (by default it analyzes 100 multi-table features)\n",
    "    khc = KhiopsClassifier()\n",
    "    khc.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the class on the test dataset\n",
    "    y_test_pred = khc.predict(X_test)\n",
    "    print(\"Predicted classes (first 10):\")\n",
    "    print(y_test_pred[:10])\n",
    "    print(\"---\")\n",
    "\n",
    "    # Predict the class probability on the test dataset\n",
    "    y_test_probas = khc.predict_proba(X_test)\n",
    "    print(f\"Class order: {khc.classes_}\")\n",
    "    print(\"Predicted class probabilities (first 10):\")\n",
    "    print(y_test_probas[:10])\n",
    "    print(\"---\")\n",
    "\n",
    "    # Evaluate accuracy and auc metrics on the test dataset\n",
    "    test_accuracy = metrics.accuracy_score(y_test, y_test_pred)\n",
    "    test_auc = metrics.roc_auc_score(y_test, y_test_probas[:, 1])\n",
    "    print(f\"Test accuracy = {test_accuracy}\")\n",
    "    print(f\"Test auc      = {test_auc}\")\n",
    "\n",
    "#Run sample\n",
    "khiops_classifier_multitable_star()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def khiops_classifier_multitable_snowflake():\n",
    "    \"\"\"Trains a `.KhiopsClassifier` on a snowflake multi-table dataset\n",
    "\n",
    "    .. note::\n",
    "        For simplicity we train from the whole dataset. To assess the performance one\n",
    "        usually splits the dataset into train and test subsets.\n",
    "\n",
    "    \"\"\"\n",
    "    # Load the dataset tables into dataframes\n",
    "    accidents_dataset_path = path.join(kh.get_samples_dir(), \"Accidents\")\n",
    "    accidents_df = pd.read_csv(\n",
    "        path.join(accidents_dataset_path, \"Accidents.txt\"),\n",
    "        sep=\"\\t\",\n",
    "        encoding=\"latin1\",\n",
    "    )\n",
    "    users_df = pd.read_csv(\n",
    "        path.join(accidents_dataset_path, \"Users.txt\"), sep=\"\\t\", encoding=\"latin1\"\n",
    "    )\n",
    "    vehicles_df = pd.read_csv(\n",
    "        path.join(accidents_dataset_path, \"Vehicles.txt\"), sep=\"\\t\", encoding=\"latin1\"\n",
    "    )\n",
    "\n",
    "    # Build the multitable input X\n",
    "    # Note: We discard the \"Gravity\" field from the \"Users\" table as it was used to\n",
    "    # build the target column\n",
    "    X = {\n",
    "        \"main_table\": \"Accidents\",\n",
    "        \"tables\": {\n",
    "            \"Accidents\": (accidents_df, \"AccidentId\"),\n",
    "            \"Vehicles\": (vehicles_df, [\"AccidentId\", \"VehicleId\"]),\n",
    "            \"Users\": (users_df.drop(\"Gravity\", axis=1), [\"AccidentId\", \"VehicleId\"]),\n",
    "        },\n",
    "        \"relations\": [\n",
    "            (\"Accidents\", \"Vehicles\"),\n",
    "            (\"Vehicles\", \"Users\"),\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Load the target variable from the AccidentsSummary dataset\n",
    "    y = pd.read_csv(\n",
    "        path.join(kh.get_samples_dir(), \"AccidentsSummary\", \"Accidents.txt\"),\n",
    "        sep=\"\\t\",\n",
    "        encoding=\"latin1\",\n",
    "    )[\"Gravity\"]\n",
    "\n",
    "    # Train the classifier (by default it creates 1000 multi-table features)\n",
    "    khc = KhiopsClassifier(n_trees=0)\n",
    "    khc.fit(X, y)\n",
    "\n",
    "    # Predict the class on the test dataset\n",
    "    y_pred = khc.predict(X)\n",
    "    print(\"Predicted classes (first 10):\")\n",
    "    print(y_pred[:10])\n",
    "    print(\"---\")\n",
    "\n",
    "    # Predict the class probability on the train dataset\n",
    "    y_probas = khc.predict_proba(X)\n",
    "    print(f\"Class order: {khc.classes_}\")\n",
    "    print(\"Predicted class probabilities (first 10):\")\n",
    "    print(y_probas[:10])\n",
    "    print(\"---\")\n",
    "\n",
    "    # Evaluate accuracy and auc metrics on the train dataset\n",
    "    train_accuracy = metrics.accuracy_score(y_pred, y)\n",
    "    train_auc = metrics.roc_auc_score(y, y_probas[:, 1])\n",
    "    print(f\"Train accuracy = {train_accuracy}\")\n",
    "    print(f\"Train auc      = {train_auc}\")\n",
    "\n",
    "#Run sample\n",
    "khiops_classifier_multitable_snowflake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def khiops_classifier_pickle():\n",
    "    \"\"\"Shows the serialization and deserialization of a `.KhiopsClassifier`\"\"\"\n",
    "    # Load the dataset into a pandas dataframe\n",
    "    iris_path = path.join(kh.get_samples_dir(), \"Iris\", \"Iris.txt\")\n",
    "    iris_df = pd.read_csv(iris_path, sep=\"\\t\")\n",
    "\n",
    "    # Train the model with the whole dataset\n",
    "    X = iris_df.drop([\"Class\"], axis=1)\n",
    "    y = iris_df[\"Class\"]\n",
    "    khc = KhiopsClassifier()\n",
    "    khc.fit(X, y)\n",
    "\n",
    "    # Create/clean the output directory\n",
    "    results_dir = path.join(\"kh_samples\", \"khiops_classifier_pickle\")\n",
    "    khc_pickle_path = path.join(results_dir, \"khiops_classifier.pkl\")\n",
    "    if path.exists(khc_pickle_path):\n",
    "        os.remove(khc_pickle_path)\n",
    "    else:\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # Pickle its content to a file\n",
    "    with open(khc_pickle_path, \"wb\") as khc_pickle_write_file:\n",
    "        pickle.dump(khc, khc_pickle_write_file)\n",
    "\n",
    "    # Unpickle it\n",
    "    with open(khc_pickle_path, \"rb\") as khc_pickle_file:\n",
    "        new_khc = pickle.load(khc_pickle_file)\n",
    "\n",
    "    # Make some predictions on the training dataset with the unpickled classifier\n",
    "    new_khc.predict(X)\n",
    "    y_predicted = new_khc.predict(X)\n",
    "    print(\"Predicted classes (first 10):\")\n",
    "    print(y_predicted[:10])\n",
    "    print(\"---\")\n",
    "\n",
    "#Run sample\n",
    "khiops_classifier_pickle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def khiops_regressor():\n",
    "    \"\"\"Trains a `.KhiopsRegressor` on a monotable dataframe\"\"\"\n",
    "    # Load the dataset into a pandas dataframe\n",
    "    adult_path = path.join(kh.get_samples_dir(), \"Adult\", \"Adult.txt\")\n",
    "    adult_df = pd.read_csv(adult_path, sep=\"\\t\")\n",
    "\n",
    "    # Split the whole dataframe into train and test (40%-60% for speed)\n",
    "    adult_train_df, adult_test_df = train_test_split(\n",
    "        adult_df, test_size=0.6, random_state=1\n",
    "    )\n",
    "\n",
    "    # Split the dataset into:\n",
    "    # - the X feature table\n",
    "    # - the y target vector (\"age\" column)\n",
    "    X_train = adult_train_df.drop(\"age\", axis=1)\n",
    "    X_test = adult_test_df.drop(\"age\", axis=1)\n",
    "    y_train = adult_train_df[\"age\"]\n",
    "    y_test = adult_test_df[\"age\"]\n",
    "\n",
    "    # Create the regressor object\n",
    "    khr = KhiopsRegressor()\n",
    "\n",
    "    # Train the regressor\n",
    "    khr.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the values on the test dataset\n",
    "    y_test_pred = khr.predict(X_test)\n",
    "    print(\"Predicted values for 'age' (first 10):\")\n",
    "    print(y_test_pred[:10])\n",
    "    print(\"---\")\n",
    "\n",
    "    # Evaluate R2 and MAE metrics on the test dataset\n",
    "    test_r2 = metrics.r2_score(y_test, y_test_pred)\n",
    "    test_mae = metrics.mean_absolute_error(y_test, y_test_pred)\n",
    "    print(f\"Test R2  = {test_r2}\")\n",
    "    print(f\"Test MAE = {test_mae}\")\n",
    "\n",
    "#Run sample\n",
    "khiops_regressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def khiops_encoder():\n",
    "    \"\"\"Trains a `.KhiopsEncoder` on a monotable dataframe\n",
    "\n",
    "    The Khiops encoder is a supervised feature encoder. It discretizes numerical\n",
    "    features and groups categorical features in a way that the resulting interval/groups\n",
    "    have the highest class-purity.\n",
    "\n",
    "    .. note::\n",
    "        For simplicity we train from the whole dataset. To assess the performance one\n",
    "        usually splits the dataset into train and test subsets.\n",
    "    \"\"\"\n",
    "    # Load the dataset into a pandas dataframe\n",
    "    iris_path = path.join(kh.get_samples_dir(), \"Iris\", \"Iris.txt\")\n",
    "    iris_df = pd.read_csv(iris_path, sep=\"\\t\")\n",
    "\n",
    "    # Train the model with the whole dataset\n",
    "    X = iris_df.drop(\"Class\", axis=1)\n",
    "    y = iris_df[\"Class\"]\n",
    "\n",
    "    # Create the encoder object\n",
    "    khe = KhiopsEncoder()\n",
    "    khe.fit(X, y)\n",
    "\n",
    "    # Transform the training dataset\n",
    "    X_transformed = khe.transform(X)\n",
    "\n",
    "    # Print both the original and transformed features\n",
    "    print(\"Original:\")\n",
    "    print(X.head(10))\n",
    "    print(\"---\")\n",
    "    print(\"Encoded feature names:\")\n",
    "    print(khe.feature_names_out_)\n",
    "    print(\"Encoded data:\")\n",
    "    print(X_transformed[:10])\n",
    "    print(\"---\")\n",
    "\n",
    "#Run sample\n",
    "khiops_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def khiops_encoder_multitable_star():\n",
    "    \"\"\"Trains a `.KhiopsEncoder` on a star multi-table dataset\"\"\"\n",
    "    # Load the root table of the dataset into a pandas dataframe\n",
    "    accidents_dataset_path = path.join(kh.get_samples_dir(), \"AccidentsSummary\")\n",
    "    accidents_df = pd.read_csv(\n",
    "        path.join(accidents_dataset_path, \"Accidents.txt\"),\n",
    "        sep=\"\\t\",\n",
    "        encoding=\"latin1\",\n",
    "    )\n",
    "\n",
    "    # Obtain the root X feature table and the y target vector (\"Class\" column)\n",
    "    X_main = accidents_df.drop(\"Gravity\", axis=1)\n",
    "    y = accidents_df[\"Gravity\"]\n",
    "\n",
    "    # Load the secondary table of the dataset into a pandas dataframe\n",
    "    X_secondary = pd.read_csv(\n",
    "        path.join(accidents_dataset_path, \"Vehicles.txt\"), sep=\"\\t\"\n",
    "    )\n",
    "\n",
    "    # Create the dataset multitable specification for the train/test split\n",
    "    # We specify each table with a name and a tuple (dataframe, key_columns)\n",
    "    X_dataset = {\n",
    "        \"main_table\": \"Accidents\",\n",
    "        \"tables\": {\n",
    "            \"Accidents\": (X_main, \"AccidentId\"),\n",
    "            \"Vehicles\": (X_secondary, [\"AccidentId\", \"VehicleId\"]),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Create the KhiopsEncoder with 10 additional multitable features and fit it\n",
    "    khe = KhiopsEncoder(n_features=10)\n",
    "    khe.fit(X_dataset, y)\n",
    "\n",
    "    # Transform the train dataset\n",
    "    print(\"Encoded feature names:\")\n",
    "    print(khe.feature_names_out_)\n",
    "    print(\"Encoded data:\")\n",
    "    print(khe.transform(X_dataset)[:10])\n",
    "\n",
    "#Run sample\n",
    "khiops_encoder_multitable_star()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def khiops_encoder_multitable_snowflake():\n",
    "    \"\"\"Trains a `.KhiopsEncoder` on a snowflake multi-table dataset\n",
    "\n",
    "    .. note::\n",
    "        For simplicity we train from the whole dataset. To assess the performance\n",
    "        one usually splits the dataset into train and test subsets.\n",
    "    \"\"\"\n",
    "    # Load the tables into dataframes\n",
    "    accidents_dataset_path = path.join(kh.get_samples_dir(), \"Accidents\")\n",
    "    accidents_df = pd.read_csv(\n",
    "        path.join(accidents_dataset_path, \"Accidents.txt\"), sep=\"\\t\", encoding=\"latin1\"\n",
    "    )\n",
    "    users_df = pd.read_csv(\n",
    "        path.join(accidents_dataset_path, \"Users.txt\"), sep=\"\\t\", encoding=\"latin1\"\n",
    "    )\n",
    "    vehicles_df = pd.read_csv(\n",
    "        path.join(accidents_dataset_path, \"Vehicles.txt\"), sep=\"\\t\", encoding=\"latin1\"\n",
    "    )\n",
    "\n",
    "    # Build the multitable input X\n",
    "    # Note: We discard the \"Gravity\" field from the \"Users\" table as it was used to\n",
    "    # build the target column\n",
    "    X = {\n",
    "        \"main_table\": \"Accidents\",\n",
    "        \"tables\": {\n",
    "            \"Accidents\": (accidents_df, \"AccidentId\"),\n",
    "            \"Vehicles\": (vehicles_df, [\"AccidentId\", \"VehicleId\"]),\n",
    "            \"Users\": (users_df.drop(\"Gravity\", axis=1), [\"AccidentId\", \"VehicleId\"]),\n",
    "        },\n",
    "        \"relations\": [\n",
    "            (\"Accidents\", \"Vehicles\"),\n",
    "            (\"Vehicles\", \"Users\"),\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Load the target variable from the AccidentsSummary dataset\n",
    "    y = pd.read_csv(\n",
    "        path.join(kh.get_samples_dir(), \"AccidentsSummary\", \"Accidents.txt\"),\n",
    "        sep=\"\\t\",\n",
    "        encoding=\"latin1\",\n",
    "    )[\"Gravity\"]\n",
    "\n",
    "    # Create the KhiopsEncoder with 10 additional multitable features and fit it\n",
    "    khe = KhiopsEncoder(n_features=10)\n",
    "    khe.fit(X, y)\n",
    "\n",
    "    # Transform the train dataset\n",
    "    print(\"Encoded feature names:\")\n",
    "    print(khe.feature_names_out_)\n",
    "    print(\"Encoded data:\")\n",
    "    print(khe.transform(X)[:10])\n",
    "\n",
    "#Run sample\n",
    "khiops_encoder_multitable_snowflake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def khiops_encoder_pipeline_with_hgbc():\n",
    "    \"\"\"Chains a `.KhiopsEncoder` with a `~sklearn.ensemble.HistGradientBoostingClassifier`\"\"\"\n",
    "    # Load the dataset into a pandas dataframe\n",
    "    adult_path = path.join(kh.get_samples_dir(), \"Adult\", \"Adult.txt\")\n",
    "    adult_df = pd.read_csv(adult_path, sep=\"\\t\")\n",
    "\n",
    "    # Split the whole dataframe into train and test (70%-30%)\n",
    "    adult_train_df, adult_test_df = train_test_split(\n",
    "        adult_df, test_size=0.3, random_state=1\n",
    "    )\n",
    "\n",
    "    # Split the dataset into:\n",
    "    # - the X feature table\n",
    "    # - the y target vector (\"class\" column)\n",
    "    X_train = adult_train_df.drop(\"class\", axis=1)\n",
    "    X_test = adult_test_df.drop(\"class\", axis=1)\n",
    "    y_train = adult_train_df[\"class\"]\n",
    "    y_test = adult_test_df[\"class\"]\n",
    "\n",
    "    # Create the pipeline and fit it. Steps:\n",
    "    # - The khiops supervised column encoder, generates a full-categorical table\n",
    "    # - One hot encoder in all columns\n",
    "    # - Train the HGB classifier\n",
    "    pipe_steps = [\n",
    "        (\"khiops_enc\", KhiopsEncoder()),\n",
    "        (\"onehot_enc\", ColumnTransformer([], remainder=OneHotEncoder(sparse=False))),\n",
    "        (\"hgb_clf\", HistGradientBoostingClassifier()),\n",
    "    ]\n",
    "    pipe = Pipeline(pipe_steps)\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the classes on the test dataset\n",
    "    y_test_pred = pipe.predict(X_test)\n",
    "    print(\"Predicted classes (first 10):\")\n",
    "    print(y_test_pred[:10])\n",
    "    print(\"---\")\n",
    "\n",
    "    # Predict the class probabilities on the test dataset\n",
    "    y_test_probas = pipe.predict_proba(X_test)\n",
    "    print(\"Predicted class probabilities (first 10):\")\n",
    "    print(y_test_probas[:10])\n",
    "    print(\"---\")\n",
    "\n",
    "    # Evaluate accuracy and auc metrics on the test dataset\n",
    "    test_accuracy = metrics.accuracy_score(y_test, y_test_pred)\n",
    "    test_auc = metrics.roc_auc_score(y_test, y_test_probas[:, 1])\n",
    "    print(f\"Test accuracy = {test_accuracy}\")\n",
    "    print(f\"Test auc      = {test_auc}\")\n",
    "\n",
    "#Run sample\n",
    "khiops_encoder_pipeline_with_hgbc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def khiops_coclustering():\n",
    "    \"\"\"Trains a `.KhiopsCoclustering` on a dataframe\"\"\"\n",
    "    # Load the secondary table of the dataset into a pandas dataframe\n",
    "    splice_dataset_path = path.join(kh.get_samples_dir(), \"SpliceJunction\")\n",
    "    splice_dna_X = pd.read_csv(\n",
    "        path.join(splice_dataset_path, \"SpliceJunctionDNA.txt\"), sep=\"\\t\"\n",
    "    )\n",
    "\n",
    "    # Train with only 70% of data (for speed in this example)\n",
    "    X, _ = train_test_split(splice_dna_X, test_size=0.3, random_state=1)\n",
    "\n",
    "    # Create the KhiopsCoclustering instance\n",
    "    khcc = KhiopsCoclustering()\n",
    "\n",
    "    # Train the model with the whole dataset\n",
    "    khcc.fit(X, id_column=\"SampleId\")\n",
    "\n",
    "    # Predict the clusters in some instances\n",
    "    X_clusters = khcc.predict(X)\n",
    "    print(\"Predicted clusters (first 10)\")\n",
    "    print(X_clusters[:10])\n",
    "    print(\"---\")\n",
    "\n",
    "#Run sample\n",
    "khiops_coclustering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def khiops_coclustering_simplify():\n",
    "    \"\"\"Simplifies a `.KhiopsCoclustering` already trained on a dataframe\"\"\"\n",
    "    # Load the secondary table of the dataset into a pandas dataframe\n",
    "    splice_dataset_path = path.join(kh.get_samples_dir(), \"SpliceJunction\")\n",
    "    splice_dna_X = pd.read_csv(\n",
    "        path.join(splice_dataset_path, \"SpliceJunctionDNA.txt\"), sep=\"\\t\"\n",
    "    )\n",
    "\n",
    "    # Train with only 70% of data (for speed in this example)\n",
    "    X, _ = train_test_split(splice_dna_X, test_size=0.3, random_state=1)\n",
    "\n",
    "    # Create the KhiopsCoclustering instance\n",
    "    khcc = KhiopsCoclustering()\n",
    "\n",
    "    # Train the model with the whole dataset\n",
    "    khcc.fit(X, id_column=\"SampleId\")\n",
    "\n",
    "    # Simplify coclustering along the individual ID dimension\n",
    "    simplified_khcc = khcc.simplify(max_part_numbers={\"SampleId\": 3})\n",
    "\n",
    "    # Predict the clusters using the simplified model\n",
    "    X_clusters = simplified_khcc.predict(X)\n",
    "    print(\"Predicted clusters (only three at most)\")\n",
    "    print(X_clusters)\n",
    "    print(\"---\")\n",
    "\n",
    "#Run sample\n",
    "khiops_coclustering_simplify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def khiops_classifier_multitable_list():\n",
    "    \"\"\"Trains a KhiopsClassifier using a list dataset specification\n",
    "\n",
    "    .. warning::\n",
    "        This dataset input method is **Deprecated** and will be removed in Khiops 11.\n",
    "    \"\"\"\n",
    "    # Load the root table of the dataset into a pandas dataframe\n",
    "    accidents_dataset_path = path.join(kh.get_samples_dir(), \"AccidentsSummary\")\n",
    "    accidents_df = pd.read_csv(\n",
    "        path.join(accidents_dataset_path, \"Accidents.txt\"),\n",
    "        sep=\"\\t\",\n",
    "        encoding=\"latin1\",\n",
    "    )\n",
    "\n",
    "    # Split the root dataframe into train and test\n",
    "    accidents_train_df, accidents_test_df = train_test_split(\n",
    "        accidents_df, test_size=0.3, random_state=1\n",
    "    )\n",
    "\n",
    "    # Obtain the main X feature table and the y target vector (\"Class\" column)\n",
    "    y_train = accidents_train_df[\"Gravity\"]\n",
    "    y_test = accidents_test_df[\"Gravity\"]\n",
    "    X_train_main = accidents_train_df.drop(\"Gravity\", axis=1)\n",
    "    X_test_main = accidents_test_df.drop(\"Gravity\", axis=1)\n",
    "\n",
    "    # Load the secondary table of the dataset into a pandas dataframe\n",
    "    vehicles_df = pd.read_csv(\n",
    "        path.join(accidents_dataset_path, \"Vehicles.txt\"), sep=\"\\t\"\n",
    "    )\n",
    "\n",
    "    # Split the secondary dataframe with the keys of the splitted root dataframe\n",
    "    X_train_ids = X_train_main[\"AccidentId\"].to_frame()\n",
    "    X_test_ids = X_test_main[\"AccidentId\"].to_frame()\n",
    "    X_train_secondary = X_train_ids.merge(vehicles_df, on=\"AccidentId\")\n",
    "    X_test_secondary = X_test_ids.merge(vehicles_df, on=\"AccidentId\")\n",
    "\n",
    "    # Create the classifier specifying the key column name\n",
    "    khc = KhiopsClassifier(key=\"AccidentId\")\n",
    "\n",
    "    # Train the classifier\n",
    "    khc.fit([X_train_main, X_train_secondary], y_train)\n",
    "\n",
    "    # Predict the class on the test dataset\n",
    "    y_test_pred = khc.predict([X_test_main, X_test_secondary])\n",
    "    print(\"Predicted classes (first 10):\")\n",
    "    print(y_test_pred[:10])\n",
    "    print(\"---\")\n",
    "\n",
    "    # Predict the class probability on the test dataset\n",
    "    y_test_probas = khc.predict_proba([X_test_main, X_test_secondary])\n",
    "    print(\"Predicted class probabilities (first 10):\")\n",
    "    print(y_test_probas[:10])\n",
    "    print(\"---\")\n",
    "\n",
    "    # Evaluate accuracy and auc metrics on the test dataset\n",
    "    test_accuracy = metrics.accuracy_score(y_test, y_test_pred)\n",
    "    test_auc = metrics.roc_auc_score(y_test, y_test_probas[:, 1])\n",
    "    print(f\"Test accuracy = {test_accuracy}\")\n",
    "    print(f\"Test auc      = {test_auc}\")\n",
    "\n",
    "#Run sample\n",
    "khiops_classifier_multitable_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def khiops_classifier_multitable_star_file():\n",
    "    \"\"\"Trains a `.KhiopsClassifier` with a file path based dataset\n",
    "\n",
    "    .. warning::\n",
    "        This dataset input method is **Deprecated** and will be removed in Khiops 11.\n",
    "        If you need to handle large datasets that do not easily fit into memory then you\n",
    "        may use the `~.khiops.core` API directly, which allows to specify file paths\n",
    "        directly.\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    results_dir = path.join(\"kh_samples\", \"khiops_classifier_multitable_file\")\n",
    "    if not path.exists(\"kh_samples\"):\n",
    "        os.mkdir(\"kh_samples\")\n",
    "        os.mkdir(results_dir)\n",
    "    else:\n",
    "        if not path.exists(results_dir):\n",
    "            os.mkdir(results_dir)\n",
    "\n",
    "    # Load the root table of the dataset into a pandas dataframe\n",
    "    accidents_dataset_path = path.join(kh.get_samples_dir(), \"AccidentsSummary\")\n",
    "    accidents_df = pd.read_csv(\n",
    "        path.join(accidents_dataset_path, \"Accidents.txt\"),\n",
    "        sep=\"\\t\",\n",
    "        encoding=\"latin1\",\n",
    "    )\n",
    "\n",
    "    # Split the root dataframe into train and test\n",
    "    X_train_main, X_test_main = train_test_split(\n",
    "        accidents_df, test_size=0.3, random_state=1\n",
    "    )\n",
    "\n",
    "    # Load the secondary table of the dataset into a pandas dataframe\n",
    "    vehicles_df = pd.read_csv(\n",
    "        path.join(accidents_dataset_path, \"Vehicles.txt\"), sep=\"\\t\"\n",
    "    )\n",
    "\n",
    "    # Split the secondary dataframe with the keys of the splitted root dataframe\n",
    "    X_train_ids = X_train_main[\"AccidentId\"].to_frame()\n",
    "    X_test_ids = X_test_main[\"AccidentId\"].to_frame()\n",
    "    X_train_secondary = X_train_ids.merge(vehicles_df, on=\"AccidentId\")\n",
    "    X_test_secondary = X_test_ids.merge(vehicles_df, on=\"AccidentId\")\n",
    "\n",
    "    # Write the train and test dataset sets to disk\n",
    "    # For the test file we remove the target column from the main table\n",
    "    X_train_main_path = path.join(results_dir, \"X_train_main.txt\")\n",
    "    X_train_main.to_csv(X_train_main_path, sep=\"\\t\", header=True, index=False)\n",
    "    X_train_secondary_path = path.join(results_dir, \"X_train_secondary.txt\")\n",
    "    X_train_secondary.to_csv(X_train_secondary_path, sep=\"\\t\", header=True, index=False)\n",
    "    X_test_main_path = path.join(results_dir, \"X_test_main.txt\")\n",
    "    y_test = X_test_main.sort_values(\"AccidentId\")[\"Gravity\"]\n",
    "    X_test_main.drop(columns=\"Gravity\").to_csv(\n",
    "        X_test_main_path, sep=\"\\t\", header=True, index=False\n",
    "    )\n",
    "    X_test_secondary_path = path.join(results_dir, \"X_test_secondary.txt\")\n",
    "    X_test_secondary.to_csv(X_test_secondary_path, sep=\"\\t\", header=True, index=False)\n",
    "\n",
    "    # Define the dictionary of train\n",
    "    X_train_dataset = {\n",
    "        \"main_table\": \"Accidents\",\n",
    "        \"tables\": {\n",
    "            \"Accidents\": (X_train_main_path, \"AccidentId\"),\n",
    "            \"Vehicles\": (X_train_secondary_path, [\"AccidentId\", \"VehicleId\"]),\n",
    "        },\n",
    "        \"format\": (\"\\t\", True),\n",
    "    }\n",
    "    X_test_dataset = {\n",
    "        \"main_table\": \"Accidents\",\n",
    "        \"tables\": {\n",
    "            \"Accidents\": (X_test_main_path, \"AccidentId\"),\n",
    "            \"Vehicles\": (X_test_secondary_path, [\"AccidentId\", \"VehicleId\"]),\n",
    "        },\n",
    "        \"format\": (\"\\t\", True),\n",
    "    }\n",
    "\n",
    "    # Create the classifier and fit it\n",
    "    khc = KhiopsClassifier(output_dir=results_dir)\n",
    "    khc.fit(X_train_dataset, y=\"Gravity\")\n",
    "\n",
    "    # Predict the class in addition to the class probabilities on the test dataset\n",
    "    y_test_pred_path = khc.predict(X_test_dataset)\n",
    "    y_test_pred = pd.read_csv(y_test_pred_path, sep=\"\\t\")\n",
    "    print(\"Predicted classes (first 10):\")\n",
    "    print(y_test_pred[\"PredictedGravity\"].head(10))\n",
    "    print(\"---\")\n",
    "\n",
    "    y_test_probas_path = khc.predict_proba(X_test_dataset)\n",
    "    y_test_probas = pd.read_csv(y_test_probas_path, sep=\"\\t\")\n",
    "    proba_columns = [col for col in y_test_probas if col.startswith(\"Prob\")]\n",
    "    print(\"Predicted class probabilities (first 10):\")\n",
    "    print(y_test_probas[proba_columns].head(10))\n",
    "    print(\"---\")\n",
    "\n",
    "    # Evaluate accuracy and auc metrics on the test dataset\n",
    "    test_accuracy = metrics.accuracy_score(y_test, y_test_pred[\"PredictedGravity\"])\n",
    "    test_auc = metrics.roc_auc_score(y_test, y_test_probas[\"ProbGravityLethal\"])\n",
    "    print(f\"Test accuracy = {test_accuracy}\")\n",
    "    print(f\"Test auc      = {test_auc}\")\n",
    "\n",
    "#Run sample\n",
    "khiops_classifier_multitable_star_file()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}