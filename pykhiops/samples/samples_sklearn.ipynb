{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyKhiops samples_sklearn\n",
    "This is a notebook containing the code in the `samples_sklearn.py` script\nof the Khiops python library `pykhiops`.\n\nMake sure you have already installed the latest version of ",
    "[Khiops](http://www.khiops.com) before using this this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from os import path\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from pykhiops import core as pk\n",
    "from pykhiops.sklearn import (\n",
    "    KhiopsClassifier,\n",
    "    KhiopsCoclustering,\n",
    "    KhiopsEncoder,\n",
    "    KhiopsRegressor,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def khiops_classifier():\n",
    "    \"\"\"Trains a `.KhiopsClassifier` on a monotable dataframe\"\"\"\n",
    "    # Load the dataset into a pandas dataframe\n",
    "    adult_path = path.join(pk.get_samples_dir(), \"Adult\", \"Adult.txt\")\n",
    "    adult_df = pd.read_csv(adult_path, sep=\"\\t\")\n",
    "\n",
    "    # Split the whole dataframe into train and test (70%-30%)\n",
    "    adult_train_df, adult_test_df = train_test_split(\n",
    "        adult_df, test_size=0.3, random_state=1\n",
    "    )\n",
    "\n",
    "    # Split the dataset into:\n",
    "    # - the X feature table\n",
    "    # - the y target vector (\"class\" column)\n",
    "    X_train = adult_train_df.drop([\"class\"], axis=1)\n",
    "    X_test = adult_test_df.drop([\"class\"], axis=1)\n",
    "    y_train = adult_train_df[\"class\"]\n",
    "    y_test = adult_test_df[\"class\"]\n",
    "\n",
    "    # Create the classifier object\n",
    "    pkc = KhiopsClassifier()\n",
    "\n",
    "    # Train the classifier\n",
    "    pkc.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the classes on the test dataset\n",
    "    y_test_pred = pkc.predict(X_test)\n",
    "    print(\"Predicted classes (first 10):\")\n",
    "    print(y_test_pred[0:10])\n",
    "    print(\"---\")\n",
    "\n",
    "    # Predict the class probabilities on the test dataset\n",
    "    y_test_probas = pkc.predict_proba(X_test)\n",
    "    print(f\"Class order: {pkc.classes_}\")\n",
    "    print(\"Predicted class probabilities (first 10):\")\n",
    "    print(y_test_probas[0:10])\n",
    "    print(\"---\")\n",
    "\n",
    "    # Evaluate accuracy and auc metrics on the test dataset\n",
    "    test_accuracy = metrics.accuracy_score(y_test, y_test_pred)\n",
    "    test_auc = metrics.roc_auc_score(y_test, y_test_probas[:, 1])\n",
    "    print(f\"Test accuracy = {test_accuracy}\")\n",
    "    print(f\"Test auc      = {test_auc}\")\n",
    "\n",
    "#Run sample\n",
    "khiops_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def khiops_classifier_multiclass():\n",
    "    \"\"\"Trains a multiclass `.KhiopsClassifier` on a monotable dataframe\"\"\"\n",
    "    # Load the dataset into a pandas dataframe\n",
    "    iris_path = path.join(pk.get_samples_dir(), \"Iris\", \"Iris.txt\")\n",
    "    iris_df = pd.read_csv(iris_path, sep=\"\\t\")\n",
    "\n",
    "    # Split the whole dataframe into train and test (70%-30%)\n",
    "    iris_train_df, iris_test_df = train_test_split(\n",
    "        iris_df, test_size=0.3, random_state=1\n",
    "    )\n",
    "\n",
    "    # Split the dataset into:\n",
    "    # - the X feature table\n",
    "    # - the y target vector (\"Class\" column)\n",
    "    X_train = iris_train_df.drop(\"Class\", axis=1)\n",
    "    X_test = iris_test_df.drop(\"Class\", axis=1)\n",
    "    y_train = iris_train_df[\"Class\"]\n",
    "    y_test = iris_test_df[\"Class\"]\n",
    "\n",
    "    # Create the classifier object\n",
    "    pkc = KhiopsClassifier()\n",
    "\n",
    "    # Train the classifier\n",
    "    pkc.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the classes on the test dataset\n",
    "    y_test_pred = pkc.predict(X_test)\n",
    "    print(\"Predicted classes (first 10):\")\n",
    "    print(y_test_pred[:10])\n",
    "    print(\"---\")\n",
    "\n",
    "    # Predict the class probabilities on the test datasets\n",
    "    y_test_probas = pkc.predict_proba(X_test)\n",
    "    print(f\"Class order: {pkc.classes_}\")\n",
    "    print(\"Predicted class probabilities (first 10):\")\n",
    "    print(y_test_probas[:10])\n",
    "    print(\"---\")\n",
    "\n",
    "    # Evaluate accuracy and auc metrics on the test dataset\n",
    "    test_accuracy = metrics.accuracy_score(y_test, y_test_pred)\n",
    "    test_auc = metrics.roc_auc_score(y_test, y_test_probas, multi_class=\"ovr\")\n",
    "    print(f\"Test accuracy = {test_accuracy}\")\n",
    "    print(f\"Test auc      = {test_auc}\")\n",
    "\n",
    "#Run sample\n",
    "khiops_classifier_multiclass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def khiops_regressor():\n",
    "    \"\"\"Trains a `.KhiopsRegressor` on a monotable dataframe\"\"\"\n",
    "    # Load the dataset into a pandas dataframe\n",
    "    adult_path = path.join(pk.get_samples_dir(), \"Adult\", \"Adult.txt\")\n",
    "    adult_df = pd.read_csv(adult_path, sep=\"\\t\")\n",
    "\n",
    "    # Split the whole dataframe into train and test (40%-60% for speed)\n",
    "    adult_train_df, adult_test_df = train_test_split(\n",
    "        adult_df, test_size=0.6, random_state=1\n",
    "    )\n",
    "\n",
    "    # Split the dataset into:\n",
    "    # - the X feature table\n",
    "    # - the y target vector (\"age\" column)\n",
    "    X_train = adult_train_df.drop(\"age\", axis=1)\n",
    "    X_test = adult_test_df.drop(\"age\", axis=1)\n",
    "    y_train = adult_train_df[\"age\"]\n",
    "    y_test = adult_test_df[\"age\"]\n",
    "\n",
    "    # Create the regressor object\n",
    "    pkr = KhiopsRegressor()\n",
    "\n",
    "    # Train the regressor\n",
    "    pkr.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the values on the test dataset\n",
    "    y_test_pred = pkr.predict(X_test)\n",
    "    print(\"Predicted values for 'age' (first 10):\")\n",
    "    print(y_test_pred[:10])\n",
    "    print(\"---\")\n",
    "\n",
    "    # Evaluate R2 and MAE metrics on the test dataset\n",
    "    test_r2 = metrics.r2_score(y_test, y_test_pred)\n",
    "    test_mae = metrics.mean_absolute_error(y_test, y_test_pred)\n",
    "    print(f\"Test R2  = {test_r2}\")\n",
    "    print(f\"Test MAE = {test_mae}\")\n",
    "\n",
    "#Run sample\n",
    "khiops_regressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def khiops_classifier_multitable_star():\n",
    "    \"\"\"Trains a `.KhiopsClassifier` on a star multi-table dataset\"\"\"\n",
    "    # Load the root table of the dataset into a pandas dataframe\n",
    "    accidents_dataset_path = path.join(pk.get_samples_dir(), \"AccidentsSummary\")\n",
    "    accidents_df = pd.read_csv(\n",
    "        path.join(accidents_dataset_path, \"Accidents.txt\"),\n",
    "        sep=\"\\t\",\n",
    "        encoding=\"latin1\",\n",
    "    )\n",
    "\n",
    "    # Split the root dataframe into train and test\n",
    "    accidents_train_df, accidents_test_df = train_test_split(\n",
    "        accidents_df, test_size=0.3, random_state=1\n",
    "    )\n",
    "\n",
    "    # Obtain the main X feature table and the y target vector (\"Class\" column)\n",
    "    y_train = accidents_train_df[\"Gravity\"]\n",
    "    y_test = accidents_test_df[\"Gravity\"]\n",
    "    X_train_main = accidents_train_df.drop(\"Gravity\", axis=1)\n",
    "    X_test_main = accidents_test_df.drop(\"Gravity\", axis=1)\n",
    "\n",
    "    # Load the secondary table of the dataset into a pandas dataframe\n",
    "    vehicles_df = pd.read_csv(\n",
    "        path.join(accidents_dataset_path, \"Vehicles.txt\"), sep=\"\\t\"\n",
    "    )\n",
    "\n",
    "    # Split the secondary dataframe with the keys of the splitted root dataframe\n",
    "    X_train_ids = X_train_main[\"AccidentId\"].to_frame()\n",
    "    X_test_ids = X_test_main[\"AccidentId\"].to_frame()\n",
    "    X_train_secondary = X_train_ids.merge(vehicles_df, on=\"AccidentId\")\n",
    "    X_test_secondary = X_test_ids.merge(vehicles_df, on=\"AccidentId\")\n",
    "\n",
    "    # Create the dataset multitable specification for the train/test split\n",
    "    # We specify each table with a name and a tuple (dataframe, key_columns)\n",
    "    X_train_dataset = {\n",
    "        \"main_table\": \"Accidents\",\n",
    "        \"tables\": {\n",
    "            \"Accidents\": (X_train_main, \"AccidentId\"),\n",
    "            \"Vehicles\": (X_train_secondary, [\"AccidentId\", \"VehicleId\"]),\n",
    "        },\n",
    "    }\n",
    "    X_test_dataset = {\n",
    "        \"main_table\": \"Accidents\",\n",
    "        \"tables\": {\n",
    "            \"Accidents\": (X_test_main, \"AccidentId\"),\n",
    "            \"Vehicles\": (X_test_secondary, [\"AccidentId\", \"VehicleId\"]),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Train the classifier (by default it analyzes 100 multi-table features)\n",
    "    pkc = KhiopsClassifier()\n",
    "    pkc.fit(X_train_dataset, y_train)\n",
    "\n",
    "    # Predict the class on the test dataset\n",
    "    y_test_pred = pkc.predict(X_test_dataset)\n",
    "    print(\"Predicted classes (first 10):\")\n",
    "    print(y_test_pred[:10])\n",
    "    print(\"---\")\n",
    "\n",
    "    # Predict the class probability on the test dataset\n",
    "    y_test_probas = pkc.predict_proba(X_test_dataset)\n",
    "    print(f\"Class order: {pkc.classes_}\")\n",
    "    print(\"Predicted class probabilities (first 10):\")\n",
    "    print(y_test_probas[:10])\n",
    "    print(\"---\")\n",
    "\n",
    "    # Evaluate accuracy and auc metrics on the test dataset\n",
    "    test_accuracy = metrics.accuracy_score(y_test, y_test_pred)\n",
    "    test_auc = metrics.roc_auc_score(y_test, y_test_probas[:, 1])\n",
    "    print(f\"Test accuracy = {test_accuracy}\")\n",
    "    print(f\"Test auc      = {test_auc}\")\n",
    "\n",
    "#Run sample\n",
    "khiops_classifier_multitable_star()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def khiops_classifier_multitable_snowflake():\n",
    "    \"\"\"Trains a `.KhiopsClassifier` on a snowflake multi-table dataset\"\"\"\n",
    "    accidents_dataset_path = path.join(pk.get_samples_dir(), \"Accidents\")\n",
    "    accidents_df = pd.read_csv(\n",
    "        path.join(accidents_dataset_path, \"Accidents.txt\"), sep=\"\\t\", encoding=\"latin1\"\n",
    "    )\n",
    "    places_df = pd.read_csv(\n",
    "        path.join(accidents_dataset_path, \"Places.txt\"), sep=\"\\t\", encoding=\"latin1\"\n",
    "    )\n",
    "    users_df = pd.read_csv(\n",
    "        path.join(accidents_dataset_path, \"Users.txt\"), sep=\"\\t\", encoding=\"latin1\"\n",
    "    )\n",
    "    vehicles_df = pd.read_csv(\n",
    "        path.join(accidents_dataset_path, \"Vehicles.txt\"), sep=\"\\t\", encoding=\"latin1\"\n",
    "    )\n",
    "    label_main_table = accidents_df[\"Gravity\"]\n",
    "    X_accidents = accidents_df.drop(\"Gravity\", axis=1)\n",
    "\n",
    "    dataset_spec = {\n",
    "        \"main_table\": \"Accidents\",\n",
    "        \"tables\": {\n",
    "            \"Places\": (places_df, [\"AccidentId\"]),\n",
    "            \"Vehicles\": (vehicles_df, [\"AccidentId\", \"VehicleId\"]),\n",
    "            \"Accidents\": (X_accidents, \"AccidentId\"),\n",
    "            \"Users\": (users_df, [\"AccidentId\", \"VehicleId\"]),\n",
    "        },\n",
    "        \"relations\": [\n",
    "            (\"Accidents\", \"Places\"),\n",
    "            (\"Vehicles\", \"Users\"),\n",
    "            (\"Accidents\", \"Vehicles\"),\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Train the classifier (by default it analyzes 100 multi-table features)\n",
    "    pkc = KhiopsClassifier()\n",
    "    pkc.fit(dataset_spec, label_main_table)\n",
    "\n",
    "    # Predict the class on the test dataset\n",
    "    y_test_pred = pkc.predict(dataset_spec)\n",
    "    print(\"Predicted classes (first 10):\")\n",
    "    print(y_test_pred[:10])\n",
    "    print(\"---\")\n",
    "\n",
    "    # Predict the class probability on the test dataset\n",
    "    y_test_probas = pkc.predict_proba(dataset_spec)\n",
    "    print(f\"Class order: {pkc.classes_}\")\n",
    "    print(\"Predicted class probabilities (first 10):\")\n",
    "    print(y_test_probas[:10])\n",
    "    print(\"---\")\n",
    "\n",
    "    # Evaluate accuracy and auc metrics on the test dataset\n",
    "    test_accuracy = metrics.accuracy_score(label_main_table, y_test_pred)\n",
    "    test_auc = metrics.roc_auc_score(label_main_table, y_test_probas[:, 1])\n",
    "    print(f\"Test accuracy = {test_accuracy}\")\n",
    "    print(f\"Test auc      = {test_auc}\")\n",
    "\n",
    "#Run sample\n",
    "khiops_classifier_multitable_snowflake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def khiops_encoder():\n",
    "    \"\"\"Trains a `.KhiopsEncoder` on a monotable dataframe\n",
    "\n",
    "    The Khiops encoder is a supervised feature encoder. It discretizes numerical\n",
    "    features and groups categorical features in a way that the resulting interval/groups\n",
    "    have the highest class-purity.\n",
    "\n",
    "    .. note::\n",
    "        For simplicity we train from the whole dataset. To assess the performance one\n",
    "        usually splits the dataset into train and test subsets.\n",
    "    \"\"\"\n",
    "    # Load the dataset into a pandas dataframe\n",
    "    iris_path = path.join(pk.get_samples_dir(), \"Iris\", \"Iris.txt\")\n",
    "    iris_df = pd.read_csv(iris_path, sep=\"\\t\")\n",
    "\n",
    "    # Train the model with the whole dataset\n",
    "    X = iris_df.drop([\"Class\"], axis=1)\n",
    "    y = iris_df[\"Class\"]\n",
    "\n",
    "    # Create the encoder object\n",
    "    pke = KhiopsEncoder()\n",
    "    pke.fit(X, y)\n",
    "\n",
    "    # Transform the training dataset\n",
    "    X_transformed = pke.transform(X)\n",
    "\n",
    "    # Print both the original and transformed features\n",
    "    print(\"Original:\")\n",
    "    print(X.head(10))\n",
    "    print(\"---\")\n",
    "    print(\"Encoded feature names:\")\n",
    "    print(pke.feature_names_out_)\n",
    "    print(\"Encoded data:\")\n",
    "    print(X_transformed[:10])\n",
    "    print(\"---\")\n",
    "\n",
    "#Run sample\n",
    "khiops_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def khiops_encoder_multitable_star():\n",
    "    \"\"\"Trains a `.KhiopsEncoder` on a star multi-table dataset\"\"\"\n",
    "    # Load the root table of the dataset into a pandas dataframe\n",
    "    accidents_dataset_path = path.join(pk.get_samples_dir(), \"AccidentsSummary\")\n",
    "    accidents_df = pd.read_csv(\n",
    "        path.join(accidents_dataset_path, \"Accidents.txt\"),\n",
    "        sep=\"\\t\",\n",
    "        encoding=\"latin1\",\n",
    "    )\n",
    "\n",
    "    # Obtain the root X feature table and the y target vector (\"Class\" column)\n",
    "    y = accidents_df[\"Gravity\"]\n",
    "    X_main = accidents_df.drop(\"Gravity\", axis=1)\n",
    "\n",
    "    # Load the secondary table of the dataset into a pandas dataframe\n",
    "    X_secondary = pd.read_csv(\n",
    "        path.join(accidents_dataset_path, \"Vehicles.txt\"), sep=\"\\t\"\n",
    "    )\n",
    "\n",
    "    # Create the dataset multitable specification for the train/test split\n",
    "    # We specify each table with a name and a tuple (dataframe, key_columns)\n",
    "    X_dataset = {\n",
    "        \"main_table\": \"Accidents\",\n",
    "        \"tables\": {\n",
    "            \"Accidents\": (X_main, \"AccidentId\"),\n",
    "            \"Vehicles\": (X_secondary, [\"AccidentId\", \"VehicleId\"]),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Create the KhiopsEncoder with 10 additional multitable features and fit it\n",
    "    pke = KhiopsEncoder(n_features=10)\n",
    "    pke.fit(X_dataset, y)\n",
    "\n",
    "    # Transform the train dataset\n",
    "    print(\"Encoded feature names:\")\n",
    "    print(pke.feature_names_out_)\n",
    "    print(\"Encoded data:\")\n",
    "    print(pke.transform(X_dataset)[:10])\n",
    "\n",
    "#Run sample\n",
    "khiops_encoder_multitable_star()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def khiops_encoder_multitable_snowflake():\n",
    "    \"\"\"Trains a `.KhiopsEncoder` on a snowflake multi-table dataset\"\"\"\n",
    "\n",
    "    accidents_dataset_path = path.join(pk.get_samples_dir(), \"Accidents\")\n",
    "\n",
    "    accidents_df = pd.read_csv(\n",
    "        path.join(accidents_dataset_path, \"Accidents.txt\"), sep=\"\\t\", encoding=\"latin1\"\n",
    "    )\n",
    "    places_df = pd.read_csv(\n",
    "        path.join(accidents_dataset_path, \"Places.txt\"), sep=\"\\t\", encoding=\"latin1\"\n",
    "    )\n",
    "    users_df = pd.read_csv(\n",
    "        path.join(accidents_dataset_path, \"Users.txt\"), sep=\"\\t\", encoding=\"latin1\"\n",
    "    )\n",
    "    vehicles_df = pd.read_csv(\n",
    "        path.join(accidents_dataset_path, \"Vehicles.txt\"), sep=\"\\t\", encoding=\"latin1\"\n",
    "    )\n",
    "\n",
    "    label_main_table = accidents_df[\"Gravity\"]\n",
    "    X_accidents = accidents_df.drop(\"Gravity\", axis=1)\n",
    "\n",
    "    dataset_spec = {\n",
    "        \"main_table\": \"Accidents\",\n",
    "        \"tables\": {\n",
    "            \"Places\": (places_df, [\"AccidentId\"]),\n",
    "            \"Vehicles\": (vehicles_df, [\"AccidentId\", \"VehicleId\"]),\n",
    "            \"Accidents\": (X_accidents, \"AccidentId\"),\n",
    "            \"Users\": (users_df, [\"AccidentId\", \"VehicleId\"]),\n",
    "        },\n",
    "        \"relations\": [\n",
    "            (\"Accidents\", \"Places\"),\n",
    "            (\"Vehicles\", \"Users\"),\n",
    "            (\"Accidents\", \"Vehicles\"),\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Create the KhiopsEncoder with 10 additional multitable features and fit it\n",
    "    pke = KhiopsEncoder(n_features=10)\n",
    "    pke.fit(dataset_spec, label_main_table)\n",
    "\n",
    "    # Transform the train dataset\n",
    "    print(\"Encoded feature names:\")\n",
    "    print(pke.feature_names_out_)\n",
    "    print(\"Encoded data:\")\n",
    "    print(pke.transform(dataset_spec)[:10])\n",
    "\n",
    "#Run sample\n",
    "khiops_encoder_multitable_snowflake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def khiops_encoder_pipeline_with_hgbc():\n",
    "    \"\"\"Chains a `.KhiopsEncoder` with a `~sklearn.ensemble.HistGradientBoostingClassifier`\"\"\"\n",
    "    # Load the dataset into a pandas dataframe\n",
    "    adult_path = path.join(pk.get_samples_dir(), \"Adult\", \"Adult.txt\")\n",
    "    adult_df = pd.read_csv(adult_path, sep=\"\\t\")\n",
    "\n",
    "    # Split the whole dataframe into train and test (70%-30%)\n",
    "    adult_train_df, adult_test_df = train_test_split(\n",
    "        adult_df, test_size=0.3, random_state=1\n",
    "    )\n",
    "\n",
    "    # Split the dataset into:\n",
    "    # - the X feature table\n",
    "    # - the y target vector (\"class\" column)\n",
    "    X_train = adult_train_df.drop(\"class\", axis=1)\n",
    "    X_test = adult_test_df.drop(\"class\", axis=1)\n",
    "    y_train = adult_train_df[\"class\"]\n",
    "    y_test = adult_test_df[\"class\"]\n",
    "\n",
    "    # Create the pipeline and fit it. Steps:\n",
    "    # - The khiops supervised column encoder, generates a full-categorical table\n",
    "    # - One hot encoder in all columns\n",
    "    # - Train the HGB classifier\n",
    "    pipe_steps = [\n",
    "        (\"khiops_enc\", KhiopsEncoder()),\n",
    "        (\"onehot_enc\", ColumnTransformer([], remainder=OneHotEncoder(sparse=False))),\n",
    "        (\"hgb_clf\", HistGradientBoostingClassifier()),\n",
    "    ]\n",
    "    pipe = Pipeline(pipe_steps)\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the classes on the test dataset\n",
    "    y_test_pred = pipe.predict(X_test)\n",
    "    print(\"Predicted classes (first 10):\")\n",
    "    print(y_test_pred[:10])\n",
    "    print(\"---\")\n",
    "\n",
    "    # Predict the class probabilities on the test dataset\n",
    "    y_test_probas = pipe.predict_proba(X_test)\n",
    "    print(\"Predicted class probabilities (first 10):\")\n",
    "    print(y_test_probas[:10])\n",
    "    print(\"---\")\n",
    "\n",
    "    # Evaluate accuracy and auc metrics on the test dataset\n",
    "    test_accuracy = metrics.accuracy_score(y_test, y_test_pred)\n",
    "    test_auc = metrics.roc_auc_score(y_test, y_test_probas[:, 1])\n",
    "    print(f\"Test accuracy = {test_accuracy}\")\n",
    "    print(f\"Test auc      = {test_auc}\")\n",
    "\n",
    "#Run sample\n",
    "khiops_encoder_pipeline_with_hgbc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def khiops_coclustering():\n",
    "    \"\"\"Trains a `.KhiopsCoclustering` on a dataframe\"\"\"\n",
    "    # Load the secondary table of the dataset into a pandas dataframe\n",
    "    splice_dataset_path = path.join(pk.get_samples_dir(), \"SpliceJunction\")\n",
    "    splice_dna_X = pd.read_csv(\n",
    "        path.join(splice_dataset_path, \"SpliceJunctionDNA.txt\"), sep=\"\\t\"\n",
    "    )\n",
    "\n",
    "    # Train with only 70% of data (for speed in this example)\n",
    "    X, _ = train_test_split(splice_dna_X, test_size=0.3, random_state=1)\n",
    "\n",
    "    # Create the KhiopsCoclustering instance\n",
    "    pkcc = KhiopsCoclustering()\n",
    "\n",
    "    # Train the model with the whole dataset\n",
    "    pkcc.fit(X, id_column=\"SampleId\")\n",
    "\n",
    "    # Predict the clusters in some instances\n",
    "    X_clusters = pkcc.predict(X)\n",
    "    print(\"Predicted clusters (first 10)\")\n",
    "    print(X_clusters[:10])\n",
    "    print(\"---\")\n",
    "\n",
    "#Run sample\n",
    "khiops_coclustering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def khiops_classifier_pickle():\n",
    "    \"\"\"Shows the serialization and deserialization of a `.KhiopsClassifier`\"\"\"\n",
    "    # Load the dataset into a pandas dataframe\n",
    "    iris_path = path.join(pk.get_samples_dir(), \"Iris\", \"Iris.txt\")\n",
    "    iris_df = pd.read_csv(iris_path, sep=\"\\t\")\n",
    "\n",
    "    # Train the model with the whole dataset\n",
    "    X = iris_df.drop([\"Class\"], axis=1)\n",
    "    y = iris_df[\"Class\"]\n",
    "    pkc = KhiopsClassifier()\n",
    "    pkc.fit(X, y)\n",
    "\n",
    "    # Create/clean the output directory\n",
    "    results_dir = path.join(\"pk_samples\", \"khiops_classifier_pickle\")\n",
    "    pkc_pickle_path = path.join(results_dir, \"khiops_classifier.pkl\")\n",
    "    if path.exists(pkc_pickle_path):\n",
    "        os.remove(pkc_pickle_path)\n",
    "    else:\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # Pickle its content to a file\n",
    "    with open(pkc_pickle_path, \"wb\") as pkc_pickle_write_file:\n",
    "        pickle.dump(pkc, pkc_pickle_write_file)\n",
    "\n",
    "    # Unpickle it\n",
    "    with open(pkc_pickle_path, \"rb\") as pkc_pickle_file:\n",
    "        new_pkc = pickle.load(pkc_pickle_file)\n",
    "\n",
    "    # Make some predictions on the training dataset with the unpickled classifier\n",
    "    new_pkc.predict(X)\n",
    "    y_predicted = new_pkc.predict(X)\n",
    "    print(\"Predicted classes (first 10):\")\n",
    "    print(y_predicted[:10])\n",
    "    print(\"---\")\n",
    "\n",
    "#Run sample\n",
    "khiops_classifier_pickle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def khiops_classifier_file_multitable_star():\n",
    "    \"\"\"Trains a `.KhiopsClassifier` with a file path based dataset\n",
    "\n",
    "    .. warning::\n",
    "        This dataset input method is **Deprecated** and will be removed in pyKhiops 11.\n",
    "        If you need to handle large datasets that do not easily fit into memory then you\n",
    "        may use the `~.pykhiops.core` API directly, which allows us to specify file\n",
    "        paths directly.\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    results_dir = path.join(\"pk_samples\", \"khiops_classifier_multitable_file\")\n",
    "    if not path.exists(\"pk_samples\"):\n",
    "        os.mkdir(\"pk_samples\")\n",
    "        os.mkdir(results_dir)\n",
    "    else:\n",
    "        if not path.exists(results_dir):\n",
    "            os.mkdir(results_dir)\n",
    "\n",
    "    # Load the root table of the dataset into a pandas dataframe\n",
    "    accidents_dataset_path = path.join(pk.get_samples_dir(), \"AccidentsSummary\")\n",
    "    accidents_df = pd.read_csv(\n",
    "        path.join(accidents_dataset_path, \"Accidents.txt\"),\n",
    "        sep=\"\\t\",\n",
    "        encoding=\"latin1\",\n",
    "    )\n",
    "\n",
    "    # Split the root dataframe into train and test\n",
    "    X_train_main, X_test_main = train_test_split(\n",
    "        accidents_df, test_size=0.3, random_state=1\n",
    "    )\n",
    "\n",
    "    # Load the secondary table of the dataset into a pandas dataframe\n",
    "    vehicles_df = pd.read_csv(\n",
    "        path.join(accidents_dataset_path, \"Vehicles.txt\"), sep=\"\\t\"\n",
    "    )\n",
    "\n",
    "    # Split the secondary dataframe with the keys of the splitted root dataframe\n",
    "    X_train_ids = X_train_main[\"AccidentId\"].to_frame()\n",
    "    X_test_ids = X_test_main[\"AccidentId\"].to_frame()\n",
    "    X_train_secondary = X_train_ids.merge(vehicles_df, on=\"AccidentId\")\n",
    "    X_test_secondary = X_test_ids.merge(vehicles_df, on=\"AccidentId\")\n",
    "\n",
    "    # Write the train and test dataset sets to disk\n",
    "    # For the test file we remove the target column from the main table\n",
    "    X_train_main_path = path.join(results_dir, \"X_train_main.txt\")\n",
    "    X_train_main.to_csv(X_train_main_path, sep=\"\\t\", header=True, index=False)\n",
    "    X_train_secondary_path = path.join(results_dir, \"X_train_secondary.txt\")\n",
    "    X_train_secondary.to_csv(X_train_secondary_path, sep=\"\\t\", header=True, index=False)\n",
    "    X_test_main_path = path.join(results_dir, \"X_test_main.txt\")\n",
    "    y_test = X_test_main.sort_values(\"AccidentId\")[\"Gravity\"]\n",
    "    X_test_main.drop(columns=\"Gravity\").to_csv(\n",
    "        X_test_main_path, sep=\"\\t\", header=True, index=False\n",
    "    )\n",
    "    X_test_secondary_path = path.join(results_dir, \"X_test_secondary.txt\")\n",
    "    X_test_secondary.to_csv(X_test_secondary_path, sep=\"\\t\", header=True, index=False)\n",
    "\n",
    "    # Define the dictionary of train\n",
    "    X_train_dataset = {\n",
    "        \"main_table\": \"Accidents\",\n",
    "        \"tables\": {\n",
    "            \"Accidents\": (X_train_main_path, \"AccidentId\"),\n",
    "            \"Vehicles\": (X_train_secondary_path, [\"AccidentId\", \"VehicleId\"]),\n",
    "        },\n",
    "        \"format\": (\"\\t\", True),\n",
    "    }\n",
    "    X_test_dataset = {\n",
    "        \"main_table\": \"Accidents\",\n",
    "        \"tables\": {\n",
    "            \"Accidents\": (X_test_main_path, \"AccidentId\"),\n",
    "            \"Vehicles\": (X_test_secondary_path, [\"AccidentId\", \"VehicleId\"]),\n",
    "        },\n",
    "        \"format\": (\"\\t\", True),\n",
    "    }\n",
    "\n",
    "    # Create the classifier and fit it\n",
    "    pkc = KhiopsClassifier(output_dir=results_dir)\n",
    "    pkc.fit(X_train_dataset, y=\"Gravity\")\n",
    "\n",
    "    # Predict the class in addition to the class probabilities on the test dataset\n",
    "    y_test_pred_path = pkc.predict(X_test_dataset)\n",
    "    y_test_pred = pd.read_csv(y_test_pred_path, sep=\"\\t\")\n",
    "    print(\"Predicted classes (first 10):\")\n",
    "    print(y_test_pred[\"PredictedGravity\"].head(10))\n",
    "    print(\"---\")\n",
    "\n",
    "    y_test_probas_path = pkc.predict_proba(X_test_dataset)\n",
    "    y_test_probas = pd.read_csv(y_test_probas_path, sep=\"\\t\")\n",
    "    proba_columns = [col for col in y_test_probas if col.startswith(\"Prob\")]\n",
    "    print(\"Predicted class probabilities (first 10):\")\n",
    "    print(y_test_probas[proba_columns].head(10))\n",
    "    print(\"---\")\n",
    "\n",
    "    # Evaluate accuracy and auc metrics on the test dataset\n",
    "    test_accuracy = metrics.accuracy_score(y_test, y_test_pred[\"PredictedGravity\"])\n",
    "    test_auc = metrics.roc_auc_score(y_test, y_test_probas[\"ProbGravityLethal\"])\n",
    "    print(f\"Test accuracy = {test_accuracy}\")\n",
    "    print(f\"Test auc      = {test_auc}\")\n",
    "\n",
    "#Run sample\n",
    "khiops_classifier_file_multitable_star()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def khiops_classifier_multitable_list():\n",
    "    \"\"\"Trains a KhiopsClassifier using a list dataset specification\n",
    "\n",
    "    .. warning::\n",
    "        This dataset input method is **Deprecated** and will be removed in pyKhiops 11.\n",
    "    \"\"\"\n",
    "    # Load the root table of the dataset into a pandas dataframe\n",
    "    accidents_dataset_path = path.join(pk.get_samples_dir(), \"AccidentsSummary\")\n",
    "    accidents_df = pd.read_csv(\n",
    "        path.join(accidents_dataset_path, \"Accidents.txt\"),\n",
    "        sep=\"\\t\",\n",
    "        encoding=\"latin1\",\n",
    "    )\n",
    "\n",
    "    # Split the root dataframe into train and test\n",
    "    accidents_train_df, accidents_test_df = train_test_split(\n",
    "        accidents_df, test_size=0.3, random_state=1\n",
    "    )\n",
    "\n",
    "    # Obtain the main X feature table and the y target vector (\"Class\" column)\n",
    "    y_train = accidents_train_df[\"Gravity\"]\n",
    "    y_test = accidents_test_df[\"Gravity\"]\n",
    "    X_train_main = accidents_train_df.drop(\"Gravity\", axis=1)\n",
    "    X_test_main = accidents_test_df.drop(\"Gravity\", axis=1)\n",
    "\n",
    "    # Load the secondary table of the dataset into a pandas dataframe\n",
    "    vehicles_df = pd.read_csv(\n",
    "        path.join(accidents_dataset_path, \"Vehicles.txt\"), sep=\"\\t\"\n",
    "    )\n",
    "\n",
    "    # Split the secondary dataframe with the keys of the splitted root dataframe\n",
    "    X_train_ids = X_train_main[\"AccidentId\"].to_frame()\n",
    "    X_test_ids = X_test_main[\"AccidentId\"].to_frame()\n",
    "    X_train_secondary = X_train_ids.merge(vehicles_df, on=\"AccidentId\")\n",
    "    X_test_secondary = X_test_ids.merge(vehicles_df, on=\"AccidentId\")\n",
    "\n",
    "    # Create the classifier specifying the key column name\n",
    "    pkc = KhiopsClassifier(key=\"AccidentId\")\n",
    "\n",
    "    # Train the classifier\n",
    "    pkc.fit([X_train_main, X_train_secondary], y_train)\n",
    "\n",
    "    # Predict the class on the test dataset\n",
    "    y_test_pred = pkc.predict([X_test_main, X_test_secondary])\n",
    "    print(\"Predicted classes (first 10):\")\n",
    "    print(y_test_pred[:10])\n",
    "    print(\"---\")\n",
    "\n",
    "    # Predict the class probability on the test dataset\n",
    "    y_test_probas = pkc.predict_proba([X_test_main, X_test_secondary])\n",
    "    print(\"Predicted class probabilities (first 10):\")\n",
    "    print(y_test_probas[:10])\n",
    "    print(\"---\")\n",
    "\n",
    "    # Evaluate accuracy and auc metrics on the test dataset\n",
    "    test_accuracy = metrics.accuracy_score(y_test, y_test_pred)\n",
    "    test_auc = metrics.roc_auc_score(y_test, y_test_probas[:, 1])\n",
    "    print(f\"Test accuracy = {test_accuracy}\")\n",
    "    print(f\"Test auc      = {test_auc}\")\n",
    "\n",
    "#Run sample\n",
    "khiops_classifier_multitable_list()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}